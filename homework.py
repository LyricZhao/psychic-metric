import random
import numpy as np


def get_confusion_matrix(pred, target, cls):
    tn, fp, fn, tp = 0, 0, 0, 0
    for i in range(len(pred)):
        tn += (pred[i] != cls and target[i] != cls)
        fp += (pred[i] != cls and target[i] == cls)
        fn += (pred[i] == cls and target[i] != cls)
        tp += (pred[i] == cls and target[i] == cls)
    return tn, fp, fn, tp


def get_common_metrics(tn, fp, fn, tp):
    accuracy_score = (tn + tp) / (tn + fp + fn + tp)
    precision_score = tp / (tp + fp)
    recall_score = tp / (tp + fn)
    if precision_score == 0 and recall_score == 0:
        f1_score = 0
    else:
        f1_score = 2 * recall_score * precision_score / (recall_score + precision_score)
    return accuracy_score, precision_score, recall_score, f1_score


def binary_classification_metrics(pred, target):
    # The inputs will be generated by the following codes:
    # pred = np.array([random.random() for i in range(100)])
    # target = np.array([random.randint(0, 1) for i in range(100)])
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(target) and len(target) > 0
    label = np.maximum(np.sign(pred - 0.5), 0)
    tn, fp, fn, tp = get_confusion_matrix(label, target, 1)
    accuracy_score, precision_score, recall_score, f1_score = get_common_metrics(tn, fp, fn, tp)
    roc_auc_score = 0  # TODO
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


def multiclass_classification_metrics(pred, target):
    # The inputs will be generated by the following codes:
    # n = 200
    # target = np.array([random.randint(0, 9) for i in range(n)])
    # pred = np.array([random.randint(0, 9) for i in range(n)])
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(target) and len(target) > 0
    classes = np.unique(np.concatenate((pred, target)))
    accuracy_score = np.sum(pred == target) / len(target)
    macro_precision_score, macro_recall_score, macro_f1_score = 0, 0, 0
    for cls in classes:
        tn, fp, fn, tp = get_confusion_matrix(pred, target, cls)
        _, precision_score, recall_score, f1_score = get_common_metrics(tn, fp, fn, tp)
        macro_precision_score += precision_score
        macro_recall_score += recall_score
        macro_f1_score += f1_score
    macro_precision_score /= len(target)
    macro_recall_score /= len(target)
    macro_f1_score /= len(target)
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, macro_precision_score, macro_recall_score, macro_f1_score
    # Comment: What is the micro scores in this case?


def multilabel_classification_metrics(pred, target):
    # Binary classification for 4 different labels.
    # The inputs will be generated by the following codes:
    # n = 300
    # target = np.array([[random.randint(0,1) for j in range(4)] for i in range(n)])
    # pred = np.array([[random.random() for j in range(4)] for i in range(n)])
    # =====================   Here to compute the metrics   ======================== #
    # Hint: compute the confusion matrix first
    raise NotImplementedError
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, macro_precision_score, macro_recall_score, macro_f1_score, \
           micro_precision_score, micro_recall_score, micro_f1_score


def ranking_metrics(pred, rel):
    # Suppose we have n items, each with a real value rel[i] and a predicted value pred[i]
    # NDCG is Normalized Discounted cumulative gain. intro: https://en.wikipedia.org/wiki/Discounted_cumulative_gain
    # The inputs will be generated by the following codes:
    # n = 30
    # rel = [random.random() for i in range(n)]
    # pred = [random.random() for i in range(n)]
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(rel) and len(pred) > 0

    def dcg(y_score, y_true):
        discount = 1 / np.log2(np.arange(len(y_score)) + 2)
        ranking = np.argsort(y_score)[::-1]
        ranked = y_true[ranking]
        cumulative_gains = discount.dot(ranked.T)
        return np.sum(cumulative_gains)

    pred, rel = np.array(pred), np.array(rel)
    ndcg_score = dcg(pred, rel) / dcg(rel, rel)
    # ===================== END OF BLOCK ======================= #
    return ndcg_score
