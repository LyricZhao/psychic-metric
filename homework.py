import random
import numpy as np


def get_confusion_matrix(pred, target, cls):
    tn, fp, fn, tp = 0, 0, 0, 0
    for i in range(len(pred)):
        tn += (pred[i] != cls and target[i] != cls)
        fp += (pred[i] == cls and target[i] != cls)
        fn += (pred[i] != cls and target[i] == cls)
        tp += (pred[i] == cls and target[i] == cls)
    return tn, fp, fn, tp


def get_common_metrics(tn, fp, fn, tp):
    accuracy_score = (tn + tp) / (tn + fp + fn + tp)
    precision_score = tp / (tp + fp)
    recall_score = tp / (tp + fn)
    if precision_score == 0 and recall_score == 0:
        f1_score = 0
    else:
        f1_score = 2 * recall_score * precision_score / (recall_score + precision_score)
    return accuracy_score, precision_score, recall_score, f1_score


def binary_classification_metrics(pred, target):
    # The inputs will be generated by the following codes:
    # pred = np.array([random.random() for i in range(100)])
    # target = np.array([random.randint(0, 1) for i in range(100)])
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(target) and len(target) > 0
    prob = pred
    pred = prob >= 0.5
    tn, fp, fn, tp = get_confusion_matrix(pred, target, 1)
    accuracy_score, precision_score, recall_score, f1_score = get_common_metrics(tn, fp, fn, tp)
    cuts = np.sort(np.unique(np.concatenate(([0.0, 1.0], prob))))[::-1]
    fpr, tpr = np.zeros_like(cuts), np.zeros_like(cuts)
    for (i, threshold) in enumerate(cuts):
        pred = prob >= threshold
        tn, fp, fn, tp = get_confusion_matrix(pred, target, 1)
        fpr[i], tpr[i] = fp / (fp + tn), tp / (tp + fn)
    fpr = np.concatenate(([0], fpr, [1]))
    tpr = np.concatenate(([0], tpr, [1]))
    roc_auc_score = 0
    for i in range(1, len(cuts)):
        dx = fpr[i] - fpr[i - 1]
        assert dx >= 0
        roc_auc_score += dx * tpr[i]
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


def multiclass_classification_metrics(pred, target):
    # The inputs will be generated by the following codes:
    # n = 200
    # target = np.array([random.randint(0, 9) for i in range(n)])
    # pred = np.array([random.randint(0, 9) for i in range(n)])
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(target) and len(target) > 0
    classes = np.unique(np.concatenate((pred, target)))
    n, n_classes = len(target), len(classes)
    accuracy_score = np.sum(pred == target) / len(target)
    macro_precision_score, macro_recall_score, macro_f1_score = 0, 0, 0
    for cls in classes:
        tn, fp, fn, tp = get_confusion_matrix(pred, target, cls)
        _, precision_score, recall_score, f1_score = get_common_metrics(tn, fp, fn, tp)
        macro_precision_score += precision_score
        macro_recall_score += recall_score
        macro_f1_score += f1_score
    macro_precision_score /= n_classes
    macro_recall_score /= n_classes
    macro_f1_score /= n_classes
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, macro_precision_score, macro_recall_score, macro_f1_score
    # Comment: What is the micro scores in this case?
    # Answer: All the scores are the same as `accuracy_score`


def multilabel_classification_metrics(pred, target):
    # Binary classification for 4 different labels.
    # The inputs will be generated by the following codes:
    # n = 300
    # target = np.array([[random.randint(0,1) for j in range(4)] for i in range(n)])
    # pred = np.array([[random.random() for j in range(4)] for i in range(n)])
    # =====================   Here to compute the metrics   ======================== #
    # Hint: compute the confusion matrix first
    assert pred.shape == target.shape
    n, n_labels = pred.shape
    pred = pred >= 0.5
    # noinspection PyUnresolvedReferences
    accuracy_score = np.sum((pred == target).all(axis=1)) / n
    macro_precision_score, macro_recall_score, macro_f1_score = 0, 0, 0
    micro_tn, micro_fp, micro_fn, micro_tp = 0, 0, 0, 0
    for label in range(n_labels):
        tn, fp, fn, tp = get_confusion_matrix(pred[:, label], target[:, label], 1)
        micro_tn += tn
        micro_fp += fp
        micro_fn += fn
        micro_tp += tp
        _, precision_score, recall_score, f1_score = get_common_metrics(tn, fp, fn, tp)
        macro_precision_score += precision_score
        macro_recall_score += recall_score
        macro_f1_score += f1_score
    macro_precision_score /= n_labels
    macro_recall_score /= n_labels
    macro_f1_score /= n_labels
    _, micro_precision_score, micro_recall_score, micro_f1_score = get_common_metrics(micro_tn, micro_fp,
                                                                                      micro_fn, micro_tp)
    # ===================== END OF BLOCK ======================= #
    return accuracy_score, macro_precision_score, macro_recall_score, macro_f1_score, \
        micro_precision_score, micro_recall_score, micro_f1_score


def ranking_metrics(pred, rel):
    # Suppose we have n items, each with a real value rel[i] and a predicted value pred[i]
    # NDCG is Normalized Discounted cumulative gain. intro: https://en.wikipedia.org/wiki/Discounted_cumulative_gain
    # The inputs will be generated by the following codes:
    # n = 30
    # rel = [random.random() for i in range(n)]
    # pred = [random.random() for i in range(n)]
    # =====================   Here to compute the metrics   ======================== #
    assert len(pred) == len(rel) and len(pred) > 0

    def dcg(y_score, y_true):
        discount = 1 / np.log2(np.arange(len(y_score)) + 2)
        ranking = np.argsort(y_score)[::-1]
        ranked = y_true[ranking]
        cumulative_gains = discount.dot(ranked.T)
        return np.sum(cumulative_gains)

    pred, rel = np.array(pred), np.array(rel)
    ndcg_score = dcg(pred, rel) / dcg(rel, rel)
    # ===================== END OF BLOCK ======================= #
    return ndcg_score
